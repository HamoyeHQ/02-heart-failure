{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Heart Failure Prediction Pipeline\n",
    "\n",
    "#### Building our lightweight pipelines components using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight python components\n",
    "\n",
    "Lightweight python components do not require you to build a new container image for every code change. They're intended to use for fast iteration in notebook environment.\n",
    "\n",
    "#### Building a lightweight python component\n",
    "\n",
    "To build a component just define a stand-alone python function and then call kfp.components.func_to_container_op(func) to convert it to a component that can be used in a pipeline.\n",
    "\n",
    "There are several requirements for the function:\n",
    "\n",
    "- The function should be stand-alone. It should not use any code declared outside of the function definition. Any imports should be added inside the main function. Any helper functions should also be defined inside the main function.\n",
    "\n",
    "\n",
    "- The function can only import packages that are available in the base image. If you need to import a package that's not available you can try to find a container image that already includes the required packages. (As a workaround you can use the module subprocess to run pip install for the required package.)\n",
    "\n",
    "\n",
    "- If the function operates on numbers, the parameters need to have type hints. Supported types are [int, float, bool]. Everything else is passed as string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Python function-based components\n",
    "\n",
    "A Kubeflow Pipelines component is a self-contained set of code that performs one step in your ML workflow. A pipeline component is composed of:\n",
    "\n",
    "- The component code, which implements the logic needed to perform a step in your ML workflow.\n",
    "\n",
    "- A component specification, which defines the following:\n",
    "    - The component's metadata, its name and description.\n",
    "    - The component's interface, the component's inputs and outputs.\n",
    "    - The component's implementation, the Docker container image to run, how to pass inputs to your component code, and how to get the component's outputs.\n",
    "    \n",
    "\n",
    "Python function-based components make it easier to iterate quickly by letting you build your component code as a Python function and generating the component specification for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U --user numpy==1.19.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install or update the pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the following command to install the Kubeflow Pipelines SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "!pip3 install --user --upgrade kfp\n",
    "!pip3 install kfp --upgrade\n",
    "!pip3 install kfp --upgrade --user\n",
    "!pip3 install -U kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Restart the kernel before you proceed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after the pip install\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Check if the install was successful:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Components\n",
    "\n",
    "#### Import the kfp and kfp.components packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp                  # the Pipelines SDK. \n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from kfp.dsl.types import Integer, GCSPath, String\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/stage-f-07-heart-failure/data/out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a release experiment in the Kubeflow pipeline\n",
    "\n",
    "#### Kubeflow Pipeline requires having an Experiment before making a run. An experiment is a group of comparable runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Heart Failure Prediction Pipeline'        # Name of the experiment in the UI\n",
    "BASE_IMAGE = \"tensorflow/tensorflow:latest-gpu-py3\"    # Base image used for components in the pipeline\n",
    "\n",
    "PROJECT_NAME = \"Kubeflow-mlops-pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an instance of the kfp.Client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Python function-based components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your component's code as a standalone python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pandas import Series, DataFrame,read_csv\n",
    "    from collections import Counter\n",
    "    from sklearn.utils import shuffle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    # Read the dataset as a csv file \n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/HamoyeHQ/stage-f-07-heart-failure/master/data/heart_failure_clinical_records_dataset.csv\")\n",
    "    \n",
    "    # Re-assign the features with binary numbers to a boolean label\n",
    "    df['anaemia'] = np.where(df['anaemia'] == 1 ,True,False)\n",
    "    df['diabetes'] = np.where(df['diabetes'] == 1, True, False)\n",
    "    df['high_blood_pressure'] = np.where(df['high_blood_pressure'] == 1, True, False)\n",
    "    df['smoking'] = np.where(df['smoking'] == 1, True, False)\n",
    "    df['sex'] = np.where(df['sex'] == 1, 'Male','Female')\n",
    "    \n",
    "    \n",
    "    # prints the number of missing values in the different variables.\n",
    "    df.apply(lambda x: sum(x.isnull()),axis=0)\n",
    "    \n",
    "    #Delete row with dummy value\n",
    "    df = df.dropna(how='any',axis=0)\n",
    "    \n",
    "    #output file to path\n",
    "    # NPZ is a file format by numpy that provides storage of array data using gzip compression. \n",
    "    np.savez_compressed(f'{data_path}/preprocessed-data.npz',\n",
    "                       df = df)\n",
    "    print(\"Preprocessing Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "def Analyze(data_path):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pandas import Series, DataFrame,read_csv\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    %matplotlib inline\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    # Read the dataset as a csv file\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/HamoyeHQ/stage-f-07-heart-failure/master/data/heart_failure_clinical_records_dataset.csv\")\n",
    "    \n",
    "    # Statistical Inference from data\n",
    "    df.describe()\n",
    "    \n",
    "    # HeatMap Correlation plot\n",
    "    plt.figure(figsize = (12,8))\n",
    "    sns.heatmap(df.corr(), annot = True)\n",
    "    \n",
    "    # Split into Features and Labels\n",
    "    x = df.drop('DEATH_EVENT', axis = 1)\n",
    "    y = df['DEATH_EVENT']\n",
    "    \n",
    "    #@title Install the facets_overview pip package.\n",
    "    !pip install facets-overview\n",
    "    \n",
    "    train_data = x[0:150] \n",
    "    test_data = x[150: ]\n",
    "    \n",
    "    \n",
    "    # Create the feature stats for the datasets and stringify it.\n",
    "    import base64\n",
    "    from facets_overview.generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n",
    "\n",
    "    gfsg = GenericFeatureStatisticsGenerator()\n",
    "    proto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': train_data},\n",
    "                                  {'name': 'test', 'table': test_data}])\n",
    "    protostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n",
    "    \n",
    "    \n",
    "    # Display the facets overview visualization for this data\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "    HTML_TEMPLATE = \"\"\"\n",
    "        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
    "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\" >\n",
    "        <facets-overview id=\"elem\"></facets-overview>\n",
    "        <script>\n",
    "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
    "        </script>\"\"\"\n",
    "    html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "    display(HTML(html))\n",
    "    \n",
    "    \n",
    "    # Distingushing those that died from a factor, from those that didn't\n",
    "    fig,ax = plt.subplots(2,3,figsize=(15,8))\n",
    "    ax1,ax2,ax3,ax4, ax5, ax6 = ax.flatten()\n",
    "\n",
    "    sns.countplot(df['anaemia'], hue = df[\"DEATH_EVENT\"],ax=ax1)\n",
    "    sns.countplot(df['diabetes'],hue = df[\"DEATH_EVENT\"],ax=ax2)\n",
    "    sns.countplot(df['high_blood_pressure'],hue = df[\"DEATH_EVENT\"], ax=ax3)\n",
    "    sns.countplot(df['sex'],hue = df[\"DEATH_EVENT\"], ax=ax4)\n",
    "    sns.countplot(df['smoking'],hue = df[\"DEATH_EVENT\"], ax=ax5)\n",
    "    sns.countplot(df['DEATH_EVENT'],hue = df[\"DEATH_EVENT\"], ax=ax6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data_path):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pandas import Series, DataFrame,read_csv\n",
    "    from collections import Counter\n",
    "    from sklearn.utils import shuffle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    #load the preprocessed data\n",
    "    preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "    df = preprocessed_data['df']\n",
    "    \n",
    "    \n",
    "    # Re-engineer some features based on generally accepted medical values for those feature\n",
    "    \n",
    "    # creatinine_phosphokinase normal values ranges from 10 to 120 micrograms per liter (mcg/L)\n",
    "    def set_cpk(row):\n",
    "        if row['creatinine_phosphokinase'] >= 10 and row['creatinine_phosphokinase'] <= 120:\n",
    "            return 'Normal'\n",
    "        else:\n",
    "            return 'Abnormal'\n",
    "    df = df.assign(cp_desc = df.apply(set_cpk, axis =1))\n",
    "    \n",
    "    \n",
    "    # Range of EJECTION FRACTION for Heart Failure\n",
    "    def set_eject_fraction(row):\n",
    "        if row['ejection_fraction'] > 50 and row['ejection_fraction'] <= 75:\n",
    "            return 'Normal'\n",
    "        else:\n",
    "            return 'Abnormal'\n",
    "    df['ejection_fraction_desc'] =  df.apply(set_eject_fraction, axis =1)\n",
    "    \n",
    "    \n",
    "    # Range of PLATELETS for Male and Female\n",
    "    def set_platelets(row):\n",
    "        if row['sex'] == 'Female':\n",
    "            if row['platelets'] >= 157000 and row['platelets'] <= 371000:\n",
    "                return 'Normal'\n",
    "            else:\n",
    "                return 'Abnormal'\n",
    "        elif row['sex'] == 'Male':\n",
    "            if row['platelets'] >= 135000 and row['platelets'] <= 317000:\n",
    "                return 'Normal'\n",
    "            else:\n",
    "                return 'Abnormal'\n",
    "    df['platelets_desc'] = df.apply(set_platelets, axis = 1)\n",
    "    \n",
    "    \n",
    "    # Range of SERUM SODIUM for Heart Failure\n",
    "    def set_sodium(row):\n",
    "        if row['serum_sodium'] >= 135 and row['serum_sodium'] <= 145:\n",
    "            return 'Normal'\n",
    "        else:\n",
    "            return 'Abnormal'\n",
    "    df['sodium_desc'] = df.apply(set_sodium, axis =1)\n",
    "    \n",
    "    # Range of SERUM CREATININE for Heart Failure (Varies for male and female)\n",
    "    def set_creatinine(row):\n",
    "        if row['sex'] == 'Female':\n",
    "            if  row['serum_creatinine'] >= 0.5 and  row['serum_creatinine'] <= 1.1:\n",
    "                return 'Normal'\n",
    "            else:\n",
    "                return 'Abnormal'\n",
    "        elif row['sex'] == 'Male':\n",
    "            if  row['serum_creatinine'] >= 0.6 and row['serum_creatinine'] <= 1.2:\n",
    "                return 'Normal'\n",
    "            else:\n",
    "                return 'Abnormal'\n",
    "    df['serum_creatinine_desc'] = df.apply(set_creatinine, axis = 1)\n",
    "    \n",
    "    \n",
    "    #output file to path\n",
    "    np.savez_compressed(f'{data_path}/feature-engineered-data.npz', \n",
    "                       x=x,\n",
    "                       y=y)\n",
    "    print(\"Feature Engineering Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# z = np.random.randn(20, 10)\n",
    "# z = pd.DataFrame(z, columns=[\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\"Occupation\", \n",
    "#                              \"Relationship\", \"Race\", \"Sex\"])\n",
    "# z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = z[0: 10] \n",
    "# test_df = z[10:  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install the facets_overview pip package.\n",
    "# !pip install facets-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the feature stats for the datasets and stringify it.\n",
    "# import base64\n",
    "# from facets_overview.generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n",
    "\n",
    "# gfsg = GenericFeatureStatisticsGenerator()\n",
    "# proto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': train_df},\n",
    "#                                   {'name': 'test', 'table': test_df}])\n",
    "# protostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the facets overview visualization for this data\n",
    "# from IPython.core.display import display, HTML\n",
    "\n",
    "# HTML_TEMPLATE = \"\"\"\n",
    "#         <script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
    "#         <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\" >\n",
    "#         <facets-overview id=\"elem\"></facets-overview>\n",
    "#         <script>\n",
    "#           document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
    "#         </script>\"\"\"\n",
    "# html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "# display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_transform(data_path):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from sklearn.utils import shuffle\n",
    "    import imblearn\n",
    "    from imblearn.over_sampling import SMOTENC\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    \n",
    "    #load the Feature Engineered data\n",
    "    feature-engineered-data = np.load(f'{data_path}/feature-engineered-data.npz')\n",
    "    x = feature-engineered-data['x']\n",
    "    y = feature-engineered-data['y']\n",
    "    \n",
    "    \n",
    "    # use SMOTENC (Synthetic Minority Over-sampling Technique for Nominal and Continuous (SMOTE-NC)), \n",
    "    # for imbalance between the target class\n",
    "    smote = SMOTENC(random_state=1,categorical_features=[0,1,3,5,9,10,12,13,14,15,16])\n",
    "    x_bal, y_bal = smote.fit_sample(x, y)\n",
    "    x_bal = DataFrame(x_bal, columns = x.columns)\n",
    "    \n",
    "    # create dummy variables for the newly engineered features.\n",
    "    encode = ['sex','cp_desc','ejection_fraction_desc','platelets_desc','sodium_desc','serum_creatinine_desc']\n",
    "    x_bal = pd.get_dummies(x_bal,columns = encode, drop_first = True)\n",
    "    \n",
    "    # Seperate the dataset, Scale the data, then concatenate the data back with its labels\n",
    "    data1 = x_bal[['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']]\n",
    "    data2 = x_bal.drop(['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium',\n",
    "                        'time'], axis = 1)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data = DataFrame(scaler.fit_transform(data1), columns = data1.columns)\n",
    "    \n",
    "    # Concatenate the data back with its labels.Simply re-join the scaled data back to its labels. \n",
    "    x = pd.concat([data,data2], axis = 1)\n",
    "    \n",
    "    #output file to path\n",
    "    np.savez_compressed(f'{data_path}/scale_transform-data.npz', \n",
    "                       x=x,\n",
    "                       y=y)\n",
    "    print(\"Scale and transform Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your component returns multiple outputs, annotate your function with the typing.NamedTuple type hint and use the collections.namedtuple function return your function's outputs as a new subclass of tuple.\n",
    "\n",
    "- You can also return metadata and metrics from your function.\n",
    "\n",
    "    - Metadata helps you visualize pipeline results.\n",
    "    - Metrics help you compare pipeline runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def training(data_path, classifier_file) -> NamedTuple(\n",
    "    'TrainingOutput',\n",
    "    [\n",
    "        ('mlpipeline_ui_metadata', 'UI_metadata'),\n",
    "        ('mlpipeline_metrics', 'Metrics')\n",
    "    ]):\n",
    "    \n",
    "    # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from sklearn.utils import shuffle\n",
    "    import imblearn\n",
    "    from imblearn.over_sampling import SMOTENC\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    #load the transformed data\n",
    "    scale-transformed-data = np.load(f'{data_path}/scale_transform-data.npz')\n",
    "    x = scale-transformed-data['x']\n",
    "    y = scale-transformed-data['y']\n",
    "    \n",
    "    # split data into training and testing set\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y_bal, test_size = 0.2, random_state = 0)\n",
    "    \n",
    "    # Instantiate classifier with obtained optimum parameters for training\n",
    "    classifier = RandomForestClassifier(max_depth = 7, max_features= 'sqrt', random_state = 4, min_samples_leaf = 1,\n",
    "                                min_samples_split = 5, n_estimators = 100)\n",
    "    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    import datetime, os\n",
    "    from datetime import datetime\n",
    "    %load_ext tensorboard \n",
    "    \n",
    "    logdir = \"/home/jovyan/stage-f-07-heart-failure/pipeline/logs/\" + datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    \n",
    "    \n",
    "    # Fit to x_train and y_train\n",
    "    classifier.fit(x_train, y_train, )\n",
    "    \n",
    "    \n",
    "    # Export a sample tensorboard\n",
    "    metadata = {\n",
    "      'outputs' : [{\n",
    "        'type': 'tensorboard',\n",
    "        'source': args.job_dir,\n",
    "      }]\n",
    "    }\n",
    "    \n",
    "    # Export metrics\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'classifier-output',\n",
    "          'numberValue':  classifier,\n",
    "        }]\n",
    "    }\n",
    "          \n",
    "          \n",
    "    #output the splitted data file to path\n",
    "    np.savez_compressed(f'{data_path}/train-test-data.npz', \n",
    "                       x_train=x_train,\n",
    "                       x_test=x_test,\n",
    "                       y_train=y_train,\n",
    "                       y_test=y_test)\n",
    "    \n",
    "    # Save the classifier model to the designated \n",
    "    with open(f'{data_path}/{classifier_file}', 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "        \n",
    "    from collections import namedtuple\n",
    "    training_output = namedtuple(\n",
    "        'TrainingOutput',\n",
    "        ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return training_output(json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def model_validation(data_path, classifier_file) -> NamedTuple(\n",
    "    'ModelvalidationOutputs',\n",
    "    [\n",
    "      ('recall', float),\n",
    "      ('accuracy', float),\n",
    "      ('precision', float),\n",
    "      ('f1score', float),\n",
    "      ('mlpipeline_ui_metadata', 'UI_metadata'),\n",
    "      ('mlpipeline_metrics', 'Metrics')\n",
    "    ]):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import classification_report, recall_score, accuracy_score,precision_score, f1_score, confusion_matrix\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    #load the transformed data\n",
    "    train-test-data = np.load(f'{data_path}/train-test-data.npz')\n",
    "    x_train = train-test-data['x_train']\n",
    "    x_test  = train-test-data['x_test']\n",
    "    y_train = train-test-data['y_train']\n",
    "    y_test  = train-test-data['y_test']\n",
    "    \n",
    "    # Load the saved classifier model\n",
    "    with open(f'{data_path}/{classifier_file}', 'rb') as file:\n",
    "        classifier = pickle.load(file)\n",
    "    \n",
    "    # predict on x_test\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    \n",
    "    # Model Evaluation\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    f1score = f1_score(y_test,y_pred)\n",
    "#     print('Recall  is: {}'.format(round(recall,4)))\n",
    "#     print(\"=====================================\")\n",
    "#     print(f'Accuracy  is : {round(accuracy,4)}')\n",
    "#     print(\"=====================================\")\n",
    "#     print('Precision  is: %s' %(round(precision,4)))\n",
    "#     print(\"=====================================\")\n",
    "#     print(f'F1_score is : {round(f1score,4)}')\n",
    "\n",
    "    # Export metrics\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "        'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "        'numberValue':  accuracy, # The value of the metric. Must be a numeric value.\n",
    "        'format': \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "      },{\n",
    "        'name': 'recall-score',\n",
    "        'numberValue': recall,\n",
    "        'format': \"PERCENTAGE\",\n",
    "      },{\n",
    "        'name': 'precision-score',\n",
    "        'numberValue': precision,\n",
    "        'format': \"PERCENTAGE\",\n",
    "      },{\n",
    "        'name': 'f1score',\n",
    "        'numberValue': f1score,\n",
    "        'format': \"PERCENTAGE\",\n",
    "          \n",
    "      }]}\n",
    "    \n",
    "    \n",
    "#     with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "#       json.dump(metrics, f)\n",
    "    \n",
    "    # Classification Report table\n",
    "    report = classification_report(y_test,y_pred)\n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    metadata = {\n",
    "      'outputs' : [{\n",
    "        'type': 'table',\n",
    "        'storage': 'inline',\n",
    "        'format': 'csv',\n",
    "        'header': [x['name'] for x in schema],\n",
    "        'source': report\n",
    "      }]\n",
    "    }\n",
    "    with open('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "      json.dump(metadata, f)\n",
    "    \n",
    "    # The Report file\n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\"Report: {} \".format(report))\n",
    "    \n",
    "    #output the splitted data file to path\n",
    "    np.savez_compressed(f'{data_path}/validated-data.npz', \n",
    "                       x_test=x_test,\n",
    "                       y_test=y_test,\n",
    "                       y_pred=y_pred)\n",
    "    \n",
    "    # Save the classifier model to the designated \n",
    "    with open(f'{data_path}/{classifier_file}', 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "        \n",
    "        \n",
    "    from collections import namedtuple\n",
    "    model_eval_output = namedtuple(\n",
    "        'ModelvalidationOutputs',\n",
    "        ['accuracy', 'recall', 'precision', 'f1score', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return model_eval_output(accuracy, recall, precision, f1score, json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Confusion_matrix(data_path, classifier_file):\n",
    "    \n",
    "     # func_to_container_op requires packages to be imported inside of the function.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    \n",
    "    # Load the saved classifier model\n",
    "    with open(f'{data_path}/{classifier_file}', 'rb') as file:\n",
    "        classifier = pickle.load(file)\n",
    "        \n",
    "    # load the transformed data\n",
    "    validated-data = np.load(f'{data_path}/validated-data.npz')\n",
    "    x_test = validated-data['x_test']\n",
    "    y_test = validated-data['y_test']\n",
    "    y_pred = validated-data['y_pred']\n",
    "    \n",
    "    \n",
    "    # Confusion matrix\n",
    "    matrix = confusion_matrix(y_test,y_pred)\n",
    "    print(matrix)\n",
    "    \n",
    "    # Make confusion matrix plot\n",
    "    matrix_plot = plot_confusion_matrix(classifier, y_test, y_pred,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix for our classifier')\n",
    "    plt.show(matrix_plot)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#      metadata = {\n",
    "#     'outputs' : [{\n",
    "#       'type': 'confusion_matrix',\n",
    "#       'format': 'csv',\n",
    "#       'schema': [\n",
    "#         {'name': 'target', 'type': 'CATEGORY'},\n",
    "#         {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "#         {'name': 'count', 'type': 'NUMBER'},\n",
    "#       ],\n",
    "#       'source': <CONFUSION_MATRIX_CSV_FILE>,\n",
    "#       # Convert vocab to string because for bealean values we want \"True|False\" to match csv data.\n",
    "#       'labels': list(map(str, vocab)),\n",
    "#     }]\n",
    "#   }\n",
    "#   with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "#     json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline component from the function\n",
    "\n",
    "#### Convert the function to a pipeline operation.\n",
    "\n",
    "- Use `kfp.components.create_component_from_func` to return a factory function that you can use to create `kfp.dsl.ContainerOp` class instances for the pipeline. We also specify the base container image to run this function in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preproces lightweight components.\n",
    "preprocess_op = comp.func_to_container_op(preprocess, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the analysis lightweight components.\n",
    "analyze_op = comp.func_to_container_op(analyze, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the feature Engineering lightweight components.\n",
    "feature_engineer_op = comp.func_to_container_op(feature_engineer, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the scale and transform lightweight components.\n",
    "scale_transform_op = comp.func_to_container_op(scale_transform, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the training lightweight components.\n",
    "training_op = comp.func_to_container_op(training, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the model evaluation lightweight components.\n",
    "model_validation_op = comp.func_to_container_op(model_validation, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create the confusion matrix lightweight components.\n",
    "confusion_matrix_op = comp.func_to_container_op(confusion_matrix, base_image=BASE_IMAGE)\n",
    "\n",
    "# Create predict_classifier lightweight components.\n",
    "# training_op = comp.func_to_container_op(training, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Kubeflow Pipeline\n",
    "\n",
    "- Our next step will be to create the various components that will make up the pipeline. Define the pipeline using the *@dsl.pipeline* decorator.\n",
    "\n",
    "\n",
    "- The pipeline function is defined and includes a number of paramters that will be fed into our various components throughout execution. Kubeflow Pipelines are created decalaratively. This means that the code is not run until the pipeline is compiled.\n",
    "\n",
    "\n",
    "- A [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) can be quickly created using the [VolumeOp](https://) method to save and persist data between the components. \n",
    "   - Note that while this is a great method to use locally, you could also use a `cloud bucket` for your persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain-specific language \n",
    "# Define the Pipeline\n",
    "@dsl.pipeline(\n",
    "    name='Heart Failure Prediction Pipeline',\n",
    "    description='End-to-end training machine learning to predict mortality by heart failure.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def Heart_Failure_container_pipeline(\n",
    "    data_path: str = DATA_PATH,\n",
    "    classifier_file: str = CLASSIFIER_PATH,    \n",
    "):\n",
    "    \n",
    "    # Create a persistent volume\n",
    "    # Define volume to share data between components\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"creat_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Define Pipeline Components and dependencies\n",
    "    # We do this with ContainerOp, an object that defines a pipeline component from a container.\n",
    "    \n",
    "    # Create Heart Failure preprocessing component.\n",
    "    heart_failure_preprocessing_container = preprocess_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    # Create Heart Failure analysis component\n",
    "    heart_failure_analyze_container = analyze_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    # Create Heart Failure Feature Engineering component\n",
    "    heart_failure_feature_engineer_container = feature_engineer_op(data_path) \\\n",
    "                                                .add_pvolumes({data_path: heart_failure_preprocessing_container.pvolume})\n",
    "    \n",
    "    # Create Heart Failure Scale and transform component\n",
    "    heart_failure_scale_transform_container = scale_transform_op(data_path) \\\n",
    "                                                .add_pvolumes({data_path: heart_failure_feature_engineer_container.pvolume})\n",
    "    \n",
    "    # Create Heart Failure training component\n",
    "    heart_failure_training_container = training_op(data_path, classifier_file) \\\n",
    "                                        .add_pvolumes({data_path: heart_failure_scale_transform_container.pvolume})\n",
    "    \n",
    "    # Create Heart Failure model evaluation component\n",
    "    heart_failure_model_validation_container = model_validation_op(data_path, classifier_file) \\\n",
    "                                        .add_pvolumes({data_path: heart_failure_training_container.pvolume})\n",
    "    \n",
    "    # Create Heart Failure confusion matrix component\n",
    "    heart_failure_confusion_matrix_container = confusion_matrix_op(data_path, classifier_file) \\\n",
    "                                        .add_pvolumes({data_path: heart_failure_model_validation.pvolume})\n",
    "    \n",
    "    # Create Heart Failure ROC Curve component\n",
    "#     heart_failure_roc_container = roc_op(data_path, classifier_file) \\\n",
    "#                                         .add_pvolumes({data_path: heart_failure_model_validation.pvolume})\n",
    "\n",
    "\n",
    "    \n",
    "     # Print the result of the prediction\n",
    "    Heart_Failure_result_container = dsl.ContainerOp(\n",
    "        name=\"Heart Failure prediction\",  # the name displayed for the component execution during runtime.\n",
    "        image='library/bash:4.4.23',      # Image tag for the Docker container to be used.\n",
    "        pvolumes={data_path: heart_failure_confusion_matrix_container.pvolume}, # dictionary of paths and associated Persistent Volumes to be mounted to the container before execution.\n",
    "        arguments=['cat', f'{data_path}/classifier_result.txt'] # command to be run by the container at runtime.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and run the pipeline\n",
    "\n",
    "- Finally we feed our pipeline definition into the compiler and run it as an experiment. This will give us 2 links at the bottom that we can follow to the [Kubeflow Pipelines UI](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) where you can check logs, artifacts, inputs/outputs, and visually see the progress of your pipeline.\n",
    "\n",
    "\n",
    "- Kubeflow Pipelines lets you group pipeline runs by Experiments. You can create a new experiment, or call `kfp.Client().list_experiments()` to see existing ones. If you don't specify the experiment name, the Default experiment will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some environment variables which are to be used as inputs at various points in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'  # mount your filesystems or devices\n",
    "CLASSIFIER_PATH = 'heart_main.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_func = Heart_Failure_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=EXPERIMENT_NAME\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"classifier_file\":CLASSIFIER_PATH}\n",
    "\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,'{}.zip'.format(experiment_name))\n",
    "\n",
    "\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
